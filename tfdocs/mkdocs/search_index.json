{
    "docs": [
        {
            "location": "/", 
            "text": "TensorFlow.jl\n\n\nIntroduction\n\n\nTensorFlow.jl is a wrapper around \nTensorFlow\n, a powerful library from Google for implementing state-of-the-art deep-learning models. See \nthe intro tutorial\n from Google to get a sense of how TensorFlow works - TensorFlow.jl has a similar API to the Python TensorFlow API described in the tutorials.\n\n\nComparison to Python API\n\n\nThe wrapper sticks closely to the Python API and so should be easy to pick up for anyone used to the Python API to pick up. Most function names and arguments are semantically the same.\n\n\nSome differences:\n\n\n\n\n\n\nWhen the Python API uses an object-oriented notation like \nsession.run(node)\n, the Julia version would be \nrun(session, node)\n.\n\n\n\n\n\n\nWhen the Python API asks for a TensorFlow type such as \nTensorFloat.float32\n, instead pass in a native Julia type like \nFloat32\n.\n\n\n\n\n\n\nMany basic Julia mathematical functions are extended to take a TensorFlow node and return another node representing the delayed execution of that function. For example, \nsqrt(constant(4.0))\n will return a \nNode\n which, when evaluated, returns \n2.0\n.\n\n\n\n\n\n\nWhat functionality of TensorFlow is exposed\n\n\nCurrently, a large fraction of the computation graph-building functionality is present. This includes\n\n\n\n\n\n\nAll basic unary and binary mathematical functions, such as \nsqrt\n, \n*\n (scalar and matrix), etc.\n\n\n\n\n\n\nThe most frequently used neural network operations, including convolution, recurrent neural networks with GRU cells, and dropout.\n\n\n\n\n\n\nNeural network trainers, such as \nAdamOptimizer\n.\n\n\n\n\n\n\nBasic image-loading and resizing operations\n\n\n\n\n\n\nCurrently not wrapped, but planned for the near future:\n\n\n\n\nDistributed graph execution\n\n\nControl flow operations (\nwhile\n loops, etc)\n\n\nPyBoard graph visualization\n\n\nSaving and restoring variables during training\n\n\n\n\nLimitations\n\n\nSince the TensorFlow API is so large, not everything is currently wrapped. If you come across TensorFlow functionality provided by the Python API not available in the Julia API, please file an issue (or even better, submit a pull request).", 
            "title": "Home"
        }, 
        {
            "location": "/#tensorflowjl", 
            "text": "", 
            "title": "TensorFlow.jl"
        }, 
        {
            "location": "/#introduction", 
            "text": "TensorFlow.jl is a wrapper around  TensorFlow , a powerful library from Google for implementing state-of-the-art deep-learning models. See  the intro tutorial  from Google to get a sense of how TensorFlow works - TensorFlow.jl has a similar API to the Python TensorFlow API described in the tutorials.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#comparison-to-python-api", 
            "text": "The wrapper sticks closely to the Python API and so should be easy to pick up for anyone used to the Python API to pick up. Most function names and arguments are semantically the same.  Some differences:    When the Python API uses an object-oriented notation like  session.run(node) , the Julia version would be  run(session, node) .    When the Python API asks for a TensorFlow type such as  TensorFloat.float32 , instead pass in a native Julia type like  Float32 .    Many basic Julia mathematical functions are extended to take a TensorFlow node and return another node representing the delayed execution of that function. For example,  sqrt(constant(4.0))  will return a  Node  which, when evaluated, returns  2.0 .", 
            "title": "Comparison to Python API"
        }, 
        {
            "location": "/#what-functionality-of-tensorflow-is-exposed", 
            "text": "Currently, a large fraction of the computation graph-building functionality is present. This includes    All basic unary and binary mathematical functions, such as  sqrt ,  *  (scalar and matrix), etc.    The most frequently used neural network operations, including convolution, recurrent neural networks with GRU cells, and dropout.    Neural network trainers, such as  AdamOptimizer .    Basic image-loading and resizing operations    Currently not wrapped, but planned for the near future:   Distributed graph execution  Control flow operations ( while  loops, etc)  PyBoard graph visualization  Saving and restoring variables during training", 
            "title": "What functionality of TensorFlow is exposed"
        }, 
        {
            "location": "/#limitations", 
            "text": "Since the TensorFlow API is so large, not everything is currently wrapped. If you come across TensorFlow functionality provided by the Python API not available in the Julia API, please file an issue (or even better, submit a pull request).", 
            "title": "Limitations"
        }, 
        {
            "location": "/basic_usage/", 
            "text": "Basic usage\n\n\nusing\n \nTensorFlow\n\n\n\nsess\n \n=\n \nSession\n()\n\n\n\nx\n \n=\n \nconstant\n(\nFloat64\n[\n1\n,\n2\n])\n\n\ny\n \n=\n \nVariable\n(\nFloat64\n[\n3\n,\n4\n])\n\n\nz\n \n=\n \nplaceholder\n(\nFloat64\n)\n\n\n\nw\n \n=\n \nexp\n(\nx\n \n+\n \nz\n \n+\n \n-\ny\n)\n\n\n\nrun\n(\nsess\n,\n \ninitialize_all_variables\n())\n\n\nres\n \n=\n \nrun\n(\nsess\n,\n \nw\n,\n \nDict\n(\nz\n=\nFloat64\n[\n1\n,\n2\n]))\n\n\n@\ntest\n \nres\n[\n1\n]\n \n\u2248\n \nexp\n(\n-\n1\n)", 
            "title": "Basic usage"
        }, 
        {
            "location": "/basic_usage/#basic-usage", 
            "text": "using   TensorFlow  sess   =   Session ()  x   =   constant ( Float64 [ 1 , 2 ])  y   =   Variable ( Float64 [ 3 , 4 ])  z   =   placeholder ( Float64 )  w   =   exp ( x   +   z   +   - y )  run ( sess ,   initialize_all_variables ())  res   =   run ( sess ,   w ,   Dict ( z = Float64 [ 1 , 2 ]))  @ test   res [ 1 ]   \u2248   exp ( - 1 )", 
            "title": "Basic usage"
        }, 
        {
            "location": "/logistic/", 
            "text": "Logistic regression\n\n\nusing\n \nDistributions\n\n\nusing\n \nTensorFlow\n\n\n\n# Generate some synthetic data\n\n\nx\n \n=\n \nrandn\n(\n100\n,\n \n50\n)\n\n\nw\n \n=\n \nrandn\n(\n50\n,\n \n10\n)\n\n\ny_prob\n \n=\n \nexp\n(\nx\n*\nw\n)\n\n\ny_prob\n \n./=\n \nsum\n(\ny_prob\n,\n2\n)\n\n\n\nfunction\n draw\n(\nprobs\n)\n\n    \ny\n \n=\n \nzeros\n(\nsize\n(\nprobs\n))\n\n    \nfor\n \ni\n \nin\n \n1\n:\nsize\n(\nprobs\n,\n \n1\n)\n\n        \nidx\n \n=\n \nrand\n(\nCategorical\n(\nprobs\n[\ni\n,\n \n:]))\n\n        \ny\n[\ni\n,\n \nidx\n]\n \n=\n \n1\n\n    \nend\n\n    \nreturn\n \ny\n\n\nend\n\n\n\ny\n \n=\n \ndraw\n(\ny_prob\n)\n\n\n\n# Build the model\n\n\nsess\n \n=\n \nSession\n()\n\n\nX\n \n=\n \nplaceholder\n(\nFloat64\n)\n\n\nY_obs\n \n=\n \nplaceholder\n(\nFloat64\n)\n\n\n\nvariable_scope\n(\nlogistic_model\n,\n \ninitializer\n=\nNormal\n(\n0\n,\n \n.\n001\n))\n \ndo\n\n    \nglobal\n \nW\n \n=\n \nget_variable\n(\nweights\n,\n \n[\n50\n,\n \n10\n],\n \nFloat64\n)\n\n    \nglobal\n \nB\n \n=\n \nget_variable\n(\nbias\n,\n \n[\n10\n],\n \nFloat64\n)\n\n\nend\n\n\n\nY\n=\nnn\n.\nsoftmax\n(\nX\n*\nW\n \n+\n \nB\n)\n\n\nLoss\n \n=\n \n-\nreduce_sum\n(\nlog\n(\nY\n)\n.*\nY_obs\n)\n\n\noptimizer\n \n=\n \ntrain\n.\nAdamOptimizer\n()\n\n\nminimize_op\n \n=\n \ntrain\n.\nminimize\n(\noptimizer\n,\n \nLoss\n)\n\n\n\n# Run training\n\n\nrun\n(\nsess\n,\n \ninitialize_all_variables\n())\n\n\n\nfor\n \nepoch\n \nin\n \n1\n:\n100\n\n    \ncur_loss\n,\n \n_\n \n=\n \nrun\n(\nsess\n,\n \nvcat\n(\nLoss\n,\n \nminimize_op\n),\n \nDict\n(\nX\n=\nx\n,\n \nY_obs\n=\ny\n))\n\n    \nprintln\n(@\nsprintf\n(\nCurrent loss is \n%.2f\n.\n,\n \ncur_loss\n))\n\n\nend", 
            "title": "Logistic regression"
        }, 
        {
            "location": "/logistic/#logistic-regression", 
            "text": "using   Distributions  using   TensorFlow  # Generate some synthetic data  x   =   randn ( 100 ,   50 )  w   =   randn ( 50 ,   10 )  y_prob   =   exp ( x * w )  y_prob   ./=   sum ( y_prob , 2 )  function  draw ( probs ) \n     y   =   zeros ( size ( probs )) \n     for   i   in   1 : size ( probs ,   1 ) \n         idx   =   rand ( Categorical ( probs [ i ,   :])) \n         y [ i ,   idx ]   =   1 \n     end \n     return   y  end  y   =   draw ( y_prob )  # Build the model  sess   =   Session ()  X   =   placeholder ( Float64 )  Y_obs   =   placeholder ( Float64 )  variable_scope ( logistic_model ,   initializer = Normal ( 0 ,   . 001 ))   do \n     global   W   =   get_variable ( weights ,   [ 50 ,   10 ],   Float64 ) \n     global   B   =   get_variable ( bias ,   [ 10 ],   Float64 )  end  Y = nn . softmax ( X * W   +   B )  Loss   =   - reduce_sum ( log ( Y ) .* Y_obs )  optimizer   =   train . AdamOptimizer ()  minimize_op   =   train . minimize ( optimizer ,   Loss )  # Run training  run ( sess ,   initialize_all_variables ())  for   epoch   in   1 : 100 \n     cur_loss ,   _   =   run ( sess ,   vcat ( Loss ,   minimize_op ),   Dict ( X = x ,   Y_obs = y )) \n     println (@ sprintf ( Current loss is  %.2f . ,   cur_loss ))  end", 
            "title": "Logistic regression"
        }
    ]
}